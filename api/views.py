import os 
import json
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from dateutil import parser as date_parser
import re
import torch 
from dotenv import load_dotenv

# dotenv ë¡œë“œ
load_dotenv() 

# Playwright
from playwright.sync_api import sync_playwright

from django.http import JsonResponse
from rest_framework.views import APIView 
from rest_framework.throttling import AnonRateThrottle
from django.utils.decorators import method_decorator
from django.views.decorators.csrf import csrf_exempt

from openai import OpenAI

# API í‚¤ ì„¤ì •
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# --- í—¬í¼ í•¨ìˆ˜: í…ìŠ¤íŠ¸ ì •ê·œí™” (ì œëª© ë¹„êµìš©) ---
def normalize_text(text):
    """ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ì—¬ ë¹„êµí•˜ê¸° ì‰½ê²Œ ë§Œë“¦"""
    if not text: return ""
    return re.sub(r'\s+|[^\w]', '', text)


def get_gpt_prediction(title, text):
    """GPT ëª¨ë¸ ë¶„ì„ + í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not client:
        return {"error": "API í‚¤ ì„¤ì • ì˜¤ë¥˜", "prediction": "Error", "score": 0}
        
    try:
        truncated_text = text[:3000]
        
        system_prompt = """
        ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ëŠ” 'íŒ©íŠ¸ì²´í¬ AI'ì…ë‹ˆë‹¤.
        ì œê³µëœ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.
        
        [ì‘ë‹µ í˜•ì‹]
        {
            "prediction": "True" ë˜ëŠ” "Fake",
            "score": 0~100 (ë†’ì„ìˆ˜ë¡ ì§„ì‹¤, ì •í™•í•œ ìˆ«ìë¡œ),
            "reason": "íŒë‹¨ ì´ìœ ë¥¼ í•œêµ­ì–´ë¡œ 2ë¬¸ì¥ ìš”ì•½",
            "keywords": "ê²€ìƒ‰ìš© í•µì‹¬ í‚¤ì›Œë“œ 2~3ê°œë¥¼ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•˜ì—¬ í•œ ì¤„ë¡œ ì‘ì„± (ì˜ˆ: ë¹„íŠ¸ì½”ì¸ í­ë½ ì „ë§)" 
        }
        """
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ì œëª©: {title}\në³¸ë¬¸: {truncated_text}"}
            ],
            response_format={"type": "json_object"},
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        result["model_type"] = "GPT-4o-mini"
        
        # scoreê°€ ì—†ê±°ë‚˜ ì˜ëª»ëœ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
        if "score" not in result or not isinstance(result["score"], (int, float)):
            result["score"] = 50
            
        return result
        
    except Exception as e:
        print(f"GPT Error: {e}")
        return {"error": str(e), "prediction": "Error", "score": 0}


# ê´€ë ¨ ê¸°ì‚¬ ì¶”ì¶œ (ë„¤ì´ë²„)
def get_related_articles(keyword):
    if not keyword: return []

    try:
        search_url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=1&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Add%2Cp%3Aall&is_sug_officeid=0"
        headers = {"User-Agent": "Mozilla/5.0"}
        
        response = requests.get(search_url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = []
        news_items = soup.select("div.news_wrap.api_ani_send")
        
        for item in news_items[:5]:
            try:
                title_tag = item.select_one("a.news_tit")
                if not title_tag: continue
                
                link = title_tag['href']
                title = title_tag.get_text().strip()
                
                img_tag = item.select_one("img.thumb")
                img_url = img_tag['data-lazysrc'] if img_tag and 'data-lazysrc' in img_tag.attrs else None
                if not img_url and img_tag: img_url = img_tag.get('src')

                press_tag = item.select_one("a.info.press")
                press = press_tag.get_text().strip() if press_tag else "ì•Œìˆ˜ì—†ìŒ"

                articles.append({
                    "title": title,
                    "link": link,
                    "press": press,
                    "thumbnail": img_url,
                    "source": "Naver"
                })
            except: continue     
        return articles

    except Exception as e:
        print(f"ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


# ê´€ë ¨ ê¸°ì‚¬ ì¶”ì¶œ (êµ¬ê¸€)
def get_google_news(keyword):
    """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ê¸€ ë‰´ìŠ¤ íƒ­ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not keyword:
        return []

    articles = []
    browser = None
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
                viewport={'width': 1280, 'height': 800}
            )
            page = context.new_page()
            
            url = f"https://www.google.com/search?q={keyword}&tbm=nws&hl=ko&gl=KR"
            page.goto(url, wait_until='domcontentloaded', timeout=10000)
            page.wait_for_timeout(3000) 

            news_elements = page.query_selector_all('div.SoaBEf, div.MjjYud')

            count = 0
            for item in news_elements:
                if count >= 5: break
                
                try:
                    link_tag = item.query_selector('a')
                    if not link_tag: continue
                    
                    link = link_tag.get_attribute('href')
                    if not link.startswith('http'): continue

                    title_div = item.query_selector('div[role="heading"]')
                    title = title_div.inner_text() if title_div else link_tag.inner_text()
                    
                    if not title: continue

                    press_div = item.query_selector('.MgUUmf span')
                    press = press_div.inner_text() if press_div else "Google News"

                    img_tag = item.query_selector('img')
                    img = img_tag.get_attribute('src') if img_tag else None

                    articles.append({
                        "title": title,
                        "link": link,
                        "press": press,
                        "thumbnail": img,
                        "source": "Google"
                    })
                    count += 1
                except:
                    continue
                    
    except Exception as e:
        print(f"âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ ê±´ë„ˆëœ€ (ì‚¬ìœ : {e})")
        return []
        
    finally:
        if browser:
            try: browser.close()
            except: pass
            
    return articles


# --- 1. AI ëª¨ë¸ ë¡œë”© (ë¡œì»¬) ---
from transformers import AutoTokenizer, AutoModelForSequenceClassification
MODEL_PATH = os.environ.get("MODEL_DIRECTORY", "./my_fake_news_model") 

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    model.eval()
    print(f"âœ… AI ëª¨ë¸ ë¡œë”© ì„±ê³µ ({MODEL_PATH})")
except Exception as e:
    print(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    tokenizer = None
    model = None


# --- 2. ì–¸ë¡ ì‚¬ ì‹ ë¢°ë„ DB (í™•ì¥ ë²„ì „) ---
MEDIA_TRUST_DB = {
    "KBS": {"rank": 1, "score": 95, "category": "ê³µì˜ë°©ì†¡"},
    "MBC": {"rank": 2, "score": 92, "category": "ê³µì˜ë°©ì†¡"},
    "SBS": {"rank": 3, "score": 88, "category": "ì§€ìƒíŒŒ"},
    "YTN": {"rank": 4, "score": 85, "category": "ë‰´ìŠ¤ì „ë¬¸"},
    "JTBC": {"rank": 5, "score": 82, "category": "ì¢…í¸"},
    "ì—°í•©ë‰´ìŠ¤": {"rank": 6, "score": 90, "category": "í†µì‹ ì‚¬"},
    "ë‰´ìŠ¤1": {"rank": 7, "score": 80, "category": "í†µì‹ ì‚¬"},
    "ì¡°ì„ ì¼ë³´": {"rank": 8, "score": 75, "category": "ì¢…í•©ì¼ê°„ì§€"},
    "ì¤‘ì•™ì¼ë³´": {"rank": 9, "score": 75, "category": "ì¢…í•©ì¼ê°„ì§€"},
    "ë™ì•„ì¼ë³´": {"rank": 10, "score": 75, "category": "ì¢…í•©ì¼ê°„ì§€"},
    "í•œê²¨ë ˆ": {"rank": 11, "score": 75, "category": "ì¢…í•©ì¼ê°„ì§€"},
    "ê²½í–¥ì‹ ë¬¸": {"rank": 12, "score": 75, "category": "ì¢…í•©ì¼ê°„ì§€"},
    "í•œêµ­ê²½ì œ": {"rank": 13, "score": 70, "category": "ê²½ì œì§€"},
    "ë§¤ì¼ê²½ì œ": {"rank": 14, "score": 70, "category": "ê²½ì œì§€"},
}

def get_media_trust_score(publisher_name):
    """ì–¸ë¡ ì‚¬ ì‹ ë¢°ë„ ì ìˆ˜ (0~100)"""
    for key, value in MEDIA_TRUST_DB.items():
        if key in publisher_name:
            return {
                "rank": value["rank"],
                "score": value["score"],
                "category": value["category"]
            }
    # ìˆœìœ„ê¶Œ ì™¸ ì–¸ë¡ ì‚¬ëŠ” ì¤‘ê°„ ì ìˆ˜
    return {"rank": None, "score": 60, "category": "ìˆœìœ„ê¶Œ ì™¸"}


def extract_date_from_url(url):
    match = re.search(r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})', url)
    if match: return f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
    match_compact = re.search(r'/(\d{4})(\d{2})(\d{2})/', url)
    if match_compact: return f"{match_compact.group(1)}-{match_compact.group(2)}-{match_compact.group(3)}"
    return None


# --- 3. ê¸°ì‚¬ ì œëª©/ë³¸ë¬¸ í¬ë¡¤ë§ ---
def find_article_content(soup):
    title = ""
    text = ""
    
    title_tag = soup.select_one('h2.media_end_head_headline') or soup.select_one('h3.tit_view')
    if title_tag: title = title_tag.get_text().strip()
    if not title and soup.find('h1'): title = soup.find('h1').get_text().strip()
    if not title:
        og_title = soup.find('meta', {'property': 'og:title'})
        if og_title and og_title.get('content'): title = og_title['content'].strip()

    article_body = soup.select_one('div#dic_area') or soup.select_one('div.article_view') or soup.find('article')
    if article_body:
        text = article_body.get_text(separator=" ").strip()
    else:
        paragraphs = soup.find_all('p')
        text = " ".join(p.get_text().strip() for p in paragraphs if p.get_text())

    return title, text


# --- 4. AI ì˜ˆì¸¡ (ë¡œì»¬) ---
def get_fake_news_prediction(title, text):
    """ë¡œì»¬ AI ëª¨ë¸ ì˜ˆì¸¡ (0~100 ì ìˆ˜ë¡œ ë³€í™˜)"""
    if not tokenizer or not model: 
        return {
            "error": "AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨",
            "score": 50,  # ê¸°ë³¸ê°’
            "prediction": "Unknown"
        }
    
    input_text = f"{title} [SEP] {text}"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
    
    with torch.no_grad(): 
        outputs = model(**inputs)
    
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    prob_true = probs[0][0].item()
    prob_fake = probs[0][1].item()
    
    # True í™•ë¥ ì„ ì ìˆ˜ë¡œ ë³€í™˜ (ë†’ì„ìˆ˜ë¡ ì§„ì‹¤)
    score = round(prob_true * 100, 2)
    
    return {
        "prediction": "Fake" if prob_fake > prob_true else "True",
        "score": score,  # ì§„ì‹¤ ì ìˆ˜
        "fake_percentage": round(prob_fake * 100, 2),
        "true_percentage": round(prob_true * 100, 2)
    }


# --- 5. ìê·¹ì ì¸ ë‹¨ì–´ ì²´í¬ (ì‹ ê·œ êµ¬í˜„) ---
def check_sensational_words(title, text):
    """
    í´ë¦­ë² ì´íŠ¸/ìê·¹ì  í‘œí˜„ íƒì§€
    Returns: {"score": 0~100, "detected_words": [...], "count": N}
    """
    clickbait_keywords = [
        'ì¶©ê²©', 'ê²½ì•…', 'ë°œì¹µ', 'ê¸´ê¸‰', 'ì†ë³´', 'ëŒ€ë°•', 'ì‹¤í™”',
        'í­ë¡œ', 'ë…¼ë€', 'ì—­ëŒ€ê¸‰', 'ì´ˆìœ ', 'ì‚¬ìƒìµœëŒ€', 'ìµœì•…',
        'ë°˜ì „', 'ê²°êµ­', 'ë“œë””ì–´', 'ë¶ˆë²•', 'íŒŒë¬¸', 'ì§„ì‹¤',
        'í—', 'ë¯¸ì³¤', 'ì‹¤ì œìƒí™©', 'ëíŒì™•', 'ë ˆì „ë“œ'
    ]
    
    detected = []
    full_text = title + " " + text[:500]  # ì œëª©+ë³¸ë¬¸ ì•ë¶€ë¶„ë§Œ
    
    for word in clickbait_keywords:
        if word in full_text:
            detected.append(word)
    
    count = len(detected)
    
    # íŒ¨ë„í‹°: 1ê°œë‹¹ -10ì  (ìµœëŒ€ -50ì )
    penalty = min(count * 10, 50)
    score = max(100 - penalty, 50)
    
    return {
        "score": score,
        "detected_words": detected,
        "count": count,
        "description": f"ìê·¹ì  í‘œí˜„ {count}ê°œ ê°ì§€" if count > 0 else "ì •ìƒ"
    }


# --- 6. ê´‘ê³ ì„±/ìƒì—…ì„± ì²´í¬ (ì‹ ê·œ êµ¬í˜„) ---
def check_commercial_content(text, url):
    """
    ê´‘ê³ /í™ë³´ì„± ì½˜í…ì¸  íƒì§€
    Returns: {"score": 0~100, "detected_patterns": [...], "is_commercial": bool}
    """
    commercial_patterns = [
        r'êµ¬ë§¤í•˜[ê¸°ëŠ”]', r'í• ì¸', r'ì´ë²¤íŠ¸', r'ì¿ í°', r'\bAD\b',
        r'í˜‘ì°¬', r'ì œê³µ:', r'ë°”ë¡œê°€ê¸°', r'í´ë¦­', r'ì§€ê¸ˆ\s?ë°”ë¡œ',
        r'ë¬´ë£Œ\s?ì²´í—˜', r'ê°€ì…', r'íšŒì›', r'í¬ì¸íŠ¸', r'í˜œíƒ',
        r'http[s]?://bit\.ly', r'http[s]?://smartstore', r'coupang\.com'
    ]
    
    detected = []
    for pattern in commercial_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            detected.append(pattern)
    
    # URLì— ì‡¼í•‘ëª°/ê´‘ê³  ë„ë©”ì¸ í¬í•¨ ì—¬ë¶€
    ad_domains = ['smartstore', 'coupang', 'gmarket', '11st', 'auction']
    url_commercial = any(domain in url.lower() for domain in ad_domains)
    
    is_commercial = len(detected) >= 3 or url_commercial
    
    # ê´‘ê³ ì„±ì´ë©´ -40ì , ì•½ê°„ ì˜ì‹¬ë˜ë©´ -20ì 
    if is_commercial:
        score = 60
    elif len(detected) > 0:
        score = 80
    else:
        score = 100
    
    return {
        "score": score,
        "detected_patterns": detected,
        "is_commercial": is_commercial,
        "description": "ê´‘ê³ ì„± ì½˜í…ì¸ " if is_commercial else "ì •ìƒ"
    }


# --- 7. í¬ë¡œìŠ¤ì²´í¬ (ê´€ë ¨ ê¸°ì‚¬ì™€ ì‚¬ì‹¤ ëŒ€ì¡°) ---
def cross_check_with_related_articles(title, text, related_articles):
    """
    ê´€ë ¨ ê¸°ì‚¬ì™€ ë‚´ìš© ì¼ì¹˜ë„ë¥¼ GPTë¡œ ê²€ì¦
    Returns: {"score": 0~100, "consistency": "ë†’ìŒ/ë³´í†µ/ë‚®ìŒ", "reason": "..."}
    """
    if not client or not related_articles:
        return {
            "score": 70,  # ê¸°ë³¸ê°’ (ê²€ì¦ ë¶ˆê°€)
            "consistency": "ê²€ì¦ë¶ˆê°€",
            "reason": "ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ê±°ë‚˜ API ì˜¤ë¥˜"
        }
    
    try:
        # ê´€ë ¨ ê¸°ì‚¬ ì œëª©ë“¤ ìš”ì•½
        related_titles = "\n".join([f"- {art['title']}" for art in related_articles[:5]])
        
        prompt = f"""
ë‹¹ì‹ ì€ íŒ©íŠ¸ì²´í¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì›ë³¸ ê¸°ì‚¬ì™€ ê´€ë ¨ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš© ì¼ì¹˜ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”.

[ì›ë³¸ ê¸°ì‚¬]
ì œëª©: {title}
ë³¸ë¬¸: {text[:1000]}

[ê´€ë ¨ ê¸°ì‚¬ ì œëª©ë“¤]
{related_titles}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{
    "consistency_score": 0~100 (ê´€ë ¨ ê¸°ì‚¬ë“¤ê³¼ ë‚´ìš©ì´ ì¼ì¹˜í• ìˆ˜ë¡ ë†’ìŒ),
    "consistency_level": "ë†’ìŒ" ë˜ëŠ” "ë³´í†µ" ë˜ëŠ” "ë‚®ìŒ",
    "reason": "íŒë‹¨ ê·¼ê±° 1ë¬¸ì¥"
}}
"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.2
        )
        
        result = json.loads(response.choices[0].message.content)
        
        return {
            "score": result.get("consistency_score", 70),
            "consistency": result.get("consistency_level", "ë³´í†µ"),
            "reason": result.get("reason", "ê²€ì¦ ì™„ë£Œ")
        }
        
    except Exception as e:
        print(f"í¬ë¡œìŠ¤ì²´í¬ ì˜¤ë¥˜: {e}")
        return {
            "score": 70,
            "consistency": "ê²€ì¦ì‹¤íŒ¨",
            "reason": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }


# --- 8. ë°œí–‰ì¼ ì‹ ì„ ë„ ì ìˆ˜ (ì‹ ê·œ) ---
def calculate_date_freshness(publish_date):
    """
    ë°œí–‰ì¼ ì‹ ì„ ë„ ì ìˆ˜ (ìµœì‹ ì¼ìˆ˜ë¡ ë†’ìŒ)
    Returns: {"score": 0~100, "days_ago": N, "freshness": "ì‹ ì„ /ë³´í†µ/ì˜¤ë˜ë¨"}
    """
    try:
        if not publish_date or publish_date == "ë‚ ì§œ ì°¾ê¸° ì‹¤íŒ¨":
            return {"score": 70, "days_ago": None, "freshness": "ë¶ˆëª…"}
        
        from datetime import datetime, timezone
        
        # ë°œí–‰ì¼ íŒŒì‹±
        if isinstance(publish_date, str):
            pub_dt = date_parser.parse(publish_date)
        else:
            pub_dt = publish_date
        
        # í˜„ì¬ ì‹œê°
        now = datetime.now(timezone.utc)
        
        # ë‚ ì§œ ì°¨ì´ ê³„ì‚°
        if pub_dt.tzinfo is None:
            pub_dt = pub_dt.replace(tzinfo=timezone.utc)
        
        days_ago = (now - pub_dt).days
        
        # ì ìˆ˜ ê³„ì‚°
        if days_ago < 0:  # ë¯¸ë˜ ë‚ ì§œ (ì˜ì‹¬)
            score = 50
            freshness = "ì˜ì‹¬"
        elif days_ago <= 7:
            score = 100
            freshness = "ì‹ ì„ "
        elif days_ago <= 30:
            score = 90
            freshness = "ìµœê·¼"
        elif days_ago <= 90:
            score = 80
            freshness = "ë³´í†µ"
        elif days_ago <= 365:
            score = 70
            freshness = "ì˜¤ë˜ë¨"
        else:
            score = 60
            freshness = "ë§¤ìš° ì˜¤ë˜ë¨"
        
        return {
            "score": score,
            "days_ago": days_ago,
            "freshness": freshness
        }
        
    except Exception as e:
        print(f"ë‚ ì§œ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"score": 70, "days_ago": None, "freshness": "ì˜¤ë¥˜"}


# --- 9. ë„ë©”ì¸/ì¶œì²˜ ì²˜ë¦¬ ---
def get_domain_from_url(url):
    try:
        netloc = urlparse(url).netloc
        return netloc.replace('www.', '') if netloc.startswith('www.') else netloc
    except: return None


def find_publisher_name(soup, domain):
    try:
        og = soup.find('meta', {'property': 'og:site_name'})
        if og and og.get('content'): return og['content'].strip()
    except: pass
    return domain


def find_publish_date(soup, url):
    meta_targets = ['article:published_time', 'og:published_time', 'pubdate']
    for attr in meta_targets:
        tag = soup.find('meta', attrs={'property': attr}) or \
              soup.find('meta', attrs={'name': attr})
            
        if tag and tag.get('content'):
            try: return date_parser.parse(tag['content']).isoformat()
            except: continue
            
    url_date = extract_date_from_url(url)
    if url_date: return url_date
    return "ë‚ ì§œ ì°¾ê¸° ì‹¤íŒ¨"


# --- 10. ìµœì¢… ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ ---
def calculate_final_score(scores_dict):
    """
    ê° ì§€í‘œë³„ ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ì ìˆ˜ ì‚°ì¶œ
    
    ê°€ì¤‘ì¹˜:
    - GPT ë¶„ì„: 25%
    - AI ëª¨ë¸: 20%
    - ì–¸ë¡ ì‚¬ ì‹ ë¢°ë„: 15%
    - í¬ë¡œìŠ¤ì²´í¬: 15%
    - ìê·¹ì  í‘œí˜„: 10%
    - ê´‘ê³ ì„±: 10%
    - ë°œí–‰ì¼ ì‹ ì„ ë„: 5%
    """
    weights = {
        "gpt_score": 0.25,
        "ai_model_score": 0.20,
        "media_trust_score": 0.15,
        "cross_check_score": 0.15,
        "sensational_score": 0.10,
        "commercial_score": 0.10,
        "date_freshness_score": 0.05
    }
    
    # ê°€ì¤‘ í•©ê³„
    final = 0
    for key, weight in weights.items():
        final += scores_dict.get(key, 50) * weight
    
    # ìµœì¢… ë“±ê¸‰
    if final >= 80:
        grade = "A"
        reliability = "ì‹ ë¢°ë„ ë†’ìŒ"
    elif final >= 60:
        grade = "B"
        reliability = "ë³´í†µ"
    elif final >= 40:
        grade = "C"
        reliability = "ì£¼ì˜ í•„ìš”"
    else:
        grade = "D"
        reliability = "ì‹ ë¢°ë„ ë‚®ìŒ"
    
    return {
        "final_score": round(final, 2),
        "grade": grade,
        "reliability": reliability,
        "weights": weights
    }


# --- 11. Django View ---
@method_decorator(csrf_exempt, name='dispatch')
class AnalyzeView(APIView):
    throttle_classes = [AnonRateThrottle]

    def post(self, request, *args, **kwargs):
        # 1. URL ë° ë°ì´í„° ì¤€ë¹„
        try:
            data = json.loads(request.body)
            url_to_check = data.get('url')
        except: 
            return JsonResponse({"success": False, "error": {"message": "ì˜ëª»ëœ ìš”ì²­"}}, status=400)

        domain = get_domain_from_url(url_to_check)
        
        # 2. í¬ë¡¤ë§ (Playwright)
        html = None
        browser = None
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page(user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
                page.goto(url_to_check, wait_until='domcontentloaded', timeout=90000)  
                html = page.content()
        except Exception as e:
            return JsonResponse({"success": False, "error": {"message": f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}"}}, status=500)
        finally:
            if browser: 
                try: browser.close() 
                except: pass
        
        if not html: 
            return JsonResponse({"success": False, "error": {"message": "HTML ì¶”ì¶œ ì‹¤íŒ¨"}}, status=500)

        # 3. ë°ì´í„° ì¶”ì¶œ
        soup = BeautifulSoup(html, "html.parser")
        publisher_name = find_publisher_name(soup, domain)
        publish_date = find_publish_date(soup, url_to_check)
        title, text_content = find_article_content(soup)

        if not title or len(text_content) < 50:
            return JsonResponse({"success": False, "error": {"message": "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"}}, status=400)

        # 4. ëª¨ë“  ì§€í‘œ ë¶„ì„ ì‹¤í–‰
        print("ğŸ“Š ë¶„ì„ ì‹œì‘...")
        
        # 4-1. GPT ë¶„ì„
        gpt_result = get_gpt_prediction(title, text_content)
        gpt_score = gpt_result.get("score", 50)
        
        # 4-2. AI ëª¨ë¸ ë¶„ì„
        ai_result = get_fake_news_prediction(title, text_content)
        ai_score = ai_result.get("score", 50)
        
        # 4-3. ì–¸ë¡ ì‚¬ ì‹ ë¢°ë„
        media_trust = get_media_trust_score(publisher_name)
        media_score = media_trust.get("score", 60)
        
        # 4-4. ìê·¹ì  í‘œí˜„
        sensational = check_sensational_words(title, text_content)
        sensational_score = sensational.get("score", 100)
        
        # 4-5. ê´‘ê³ ì„±
        commercial = check_commercial_content(text_content, url_to_check)
        commercial_score = commercial.get("score", 100)
        
        # 4-6. ë°œí–‰ì¼ ì‹ ì„ ë„
        date_freshness = calculate_date_freshness(publish_date)
        date_score = date_freshness.get("score", 70)
        
        # 4-7. ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ë° í¬ë¡œìŠ¤ì²´í¬
        related_articles = []
        cross_check_result = {"score": 70, "consistency": "ê²€ì¦ë¶ˆê°€"}
        
        keywords = gpt_result.get("keywords", "")
        if keywords:
            print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {keywords}")
            naver_raw = get_related_articles(keywords)
            google_raw = get_google_news(keywords)
            all_articles = naver_raw + google_raw
            
            # ì¤‘ë³µ ì œê±°
            current_title_norm = normalize_text(title)
            filtered_list = []
            
            for item in all_articles:
                item_title_norm = normalize_text(item['title'])
                if len(item_title_norm) < 2: continue
                if current_title_norm in item_title_norm or item_title_norm in current_title_norm:
                    continue
                filtered_list.append(item)
            
            related_articles = filtered_list[:5]
            
            # í¬ë¡œìŠ¤ì²´í¬ ì‹¤í–‰
            if related_articles:
                cross_check_result = cross_check_with_related_articles(title, text_content, related_articles)
        
        cross_check_score = cross_check_result.get("score", 70)
        
        # 5. ìµœì¢… ì ìˆ˜ ê³„ì‚°
        scores_dict = {
            "gpt_score": gpt_score,
            "ai_model_score": ai_score,
            "media_trust_score": media_score,
            "cross_check_score": cross_check_score,
            "sensational_score": sensational_score,
            "commercial_score": commercial_score,
            "date_freshness_score": date_score
        }
        
        final_result = calculate_final_score(scores_dict)
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ - ìµœì¢… ì ìˆ˜: {final_result['final_score']}")
        
        # 6. ì‘ë‹µ ë°˜í™˜
        return JsonResponse({
            "success": True,
            "data": {
                "requested_url": url_to_check,
                "publisher_name": publisher_name,
                "published_date": publish_date,
                "scraped_title": title,
                
                # ê°œë³„ ì§€í‘œ ì ìˆ˜ë“¤
                "detailed_scores": {
                    "gpt_analysis": {
                        "score": gpt_score,
                        "prediction": gpt_result.get("prediction", "Unknown"),
                        "reason": gpt_result.get("reason", ""),
                        "model_type": gpt_result.get("model_type", "GPT-4o-mini")
                    },
                    "ai_model": {
                        "score": ai_score,
                        "prediction": ai_result.get("prediction", "Unknown"),
                        "fake_percentage": ai_result.get("fake_percentage", 0),
                        "true_percentage": ai_result.get("true_percentage", 0)
                    },
                    "media_trust": {
                        "score": media_score,
                        "rank": media_trust.get("rank"),
                        "category": media_trust.get("category", "ìˆœìœ„ê¶Œ ì™¸")
                    },
                    "sensational_check": {
                        "score": sensational_score,
                        "detected_words": sensational.get("detected_words", []),
                        "count": sensational.get("count", 0),
                        "description": sensational.get("description", "ì •ìƒ")
                    },
                    "commercial_check": {
                        "score": commercial_score,
                        "is_commercial": commercial.get("is_commercial", False),
                        "detected_patterns": commercial.get("detected_patterns", []),
                        "description": commercial.get("description", "ì •ìƒ")
                    },
                    "date_freshness": {
                        "score": date_score,
                        "days_ago": date_freshness.get("days_ago"),
                        "freshness": date_freshness.get("freshness", "ë¶ˆëª…")
                    },
                    "cross_check": {
                        "score": cross_check_score,
                        "consistency": cross_check_result.get("consistency", "ê²€ì¦ë¶ˆê°€"),
                        "reason": cross_check_result.get("reason", "")
                    }
                },
                
                # ìµœì¢… ì¢…í•© ì ìˆ˜
                "final_analysis": {
                    "final_score": final_result["final_score"],
                    "grade": final_result["grade"],
                    "reliability": final_result["reliability"],
                    "weights_used": final_result["weights"]
                },
                
                # ê´€ë ¨ ê¸°ì‚¬
                "related_articles": related_articles,
                "search_keywords": keywords,
                
                "cached": False
            }
        }, status=200)
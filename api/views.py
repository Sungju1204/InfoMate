import os 
import json
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from dateutil import parser as date_parser
import re
import torch 
from dotenv import load_dotenv

# dotenv ë¡œë“œ
load_dotenv() 

# Playwright
from playwright.sync_api import sync_playwright

from django.http import JsonResponse
from rest_framework.views import APIView 
from rest_framework.throttling import AnonRateThrottle
from django.utils.decorators import method_decorator
from django.views.decorators.csrf import csrf_exempt

from openai import OpenAI

# API í‚¤ ì„¤ì •
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# --- í—¬í¼ í•¨ìˆ˜: í…ìŠ¤íŠ¸ ì •ê·œí™” (ì œëª© ë¹„êµìš©) ---
def normalize_text(text):
    """ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ì—¬ ë¹„êµí•˜ê¸° ì‰½ê²Œ ë§Œë“¦"""
    if not text: return ""
    return re.sub(r'\s+|[^\w]', '', text)


def get_gpt_prediction(title, text):
    """GPT ëª¨ë¸ ë¶„ì„ + í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not client:
        return {"error": "API í‚¤ ì„¤ì • ì˜¤ë¥˜", "prediction": "Error"}
        
    try:
        truncated_text = text[:3000]
        
        system_prompt = """
        ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ëŠ” 'íŒ©íŠ¸ì²´í¬ AI'ì…ë‹ˆë‹¤.
        ì œê³µëœ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.
        
        [ì‘ë‹µ í˜•ì‹]
        {
            "prediction": "True" ë˜ëŠ” "Fake",
            "score": 0~100 (ë†’ì„ìˆ˜ë¡ ì§„ì‹¤),
            "reason": "íŒë‹¨ ì´ìœ ë¥¼ í•œêµ­ì–´ë¡œ 2ë¬¸ì¥ ìš”ì•½",
            "keywords": "ê²€ìƒ‰ìš© í•µì‹¬ í‚¤ì›Œë“œ 2~3ê°œë¥¼ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•˜ì—¬ í•œ ì¤„ë¡œ ì‘ì„± (ì˜ˆ: ë¹„íŠ¸ì½”ì¸ í­ë½ ì „ë§)" 
        }
        """
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ì œëª©: {title}\në³¸ë¬¸: {truncated_text}"}
            ],
            response_format={"type": "json_object"},
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        result["model_type"] = "GPT-4o-mini"
        return result
        
    except Exception as e:
        print(f"GPT Error: {e}")
        return {"error": str(e), "prediction": "Error"}

# ê´€ë ¨ ê¸°ì‚¬ ì¶”ì¶œ (ë„¤ì´ë²„)
def get_related_articles(keyword):
    if not keyword: return []

    try:
        search_url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=1&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Add%2Cp%3Aall&is_sug_officeid=0"
        headers = {"User-Agent": "Mozilla/5.0"}
        
        response = requests.get(search_url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = []
        news_items = soup.select("div.news_wrap.api_ani_send")
        
        # [ìˆ˜ì •] 3ê°œ -> 5ê°œë¡œ ëŠ˜ë¦¼ (ì¤‘ë³µ ì œê±° ëŒ€ë¹„)
        for item in news_items[:5]:
            try:
                title_tag = item.select_one("a.news_tit")
                if not title_tag: continue
                
                link = title_tag['href']
                title = title_tag.get_text().strip()
                
                img_tag = item.select_one("img.thumb")
                img_url = img_tag['data-lazysrc'] if img_tag and 'data-lazysrc' in img_tag.attrs else None
                if not img_url and img_tag: img_url = img_tag.get('src')

                press_tag = item.select_one("a.info.press")
                press = press_tag.get_text().strip() if press_tag else "ì•Œìˆ˜ì—†ìŒ"

                articles.append({
                    "title": title,
                    "link": link,
                    "press": press,
                    "thumbnail": img_url,
                    "source": "Naver"
                })
            except: continue     
        return articles

    except Exception as e:
        print(f"ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

# ê´€ë ¨ ê¸°ì‚¬ ì¶”ì¶œ (êµ¬ê¸€)
def get_google_news(keyword):
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ê¸€ ë‰´ìŠ¤ íƒ­ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ì—ëŸ¬ ë°©ì§€ ê°•í™”íŒ)
    """
    if not keyword:
        return []

    articles = []
    browser = None
    try:
        with sync_playwright() as p:
            # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œì§€ë§Œ ë´‡ íƒì§€ë¥¼ í”¼í•˜ê¸° ìœ„í•œ ì„¤ì •ë“¤
            browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
                viewport={'width': 1280, 'height': 800}
            )
            page = context.new_page()
            
            # êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ URL
            url = f"https://www.google.com/search?q={keyword}&tbm=nws&hl=ko&gl=KR"
            
            # íƒ€ì„ì•„ì›ƒì„ 30ì´ˆ -> 10ì´ˆë¡œ ì¤„ì—¬ì„œ ì‹¤íŒ¨í•˜ë©´ ë¹¨ë¦¬ í¬ê¸°í•˜ê²Œ í•¨ (ì „ì²´ ì‘ë‹µ ì†ë„ ìœ„í•´)
            page.goto(url, wait_until='domcontentloaded', timeout=10000)
            
            # [ìˆ˜ì •] íŠ¹ì • ID(#search)ë¥¼ ê¸°ë‹¤ë¦¬ë‹¤ê°€ ì—ëŸ¬ë‚˜ì§€ ë§ê³ , ê·¸ëƒ¥ 2~3ì´ˆ ë©ë•Œë¦¬ë©° ë¡œë”© ê¸°ë‹¤ë¦¬ê¸°
            page.wait_for_timeout(3000) 

            # êµ¬ê¸€ ë‰´ìŠ¤ ì¹´ë“œ ìš”ì†Œë“¤ ì„ íƒ (ì—¬ëŸ¬ í´ë˜ìŠ¤ëª… ì‹œë„)
            # div.SoaBEf: ì „í†µì ì¸ ë‰´ìŠ¤ ì¹´ë“œ / div.MjjYud: ìµœì‹  ë ˆì´ì•„ì›ƒ
            news_elements = page.query_selector_all('div.SoaBEf, div.MjjYud')

            count = 0
            for item in news_elements:
                if count >= 5: break # 5ê°œë§Œ ì±„ìš°ë©´ ì¤‘ë‹¨
                
                try:
                    # ë§í¬ íƒœê·¸ ì°¾ê¸°
                    link_tag = item.query_selector('a')
                    if not link_tag: continue
                    
                    link = link_tag.get_attribute('href')
                    if not link.startswith('http'): continue # ì´ìƒí•œ ë§í¬ ì œì™¸

                    # ì œëª© ì°¾ê¸° (role="heading"ì´ ê°€ì¥ ì •í™•í•¨)
                    title_div = item.query_selector('div[role="heading"]')
                    title = title_div.inner_text() if title_div else link_tag.inner_text()
                    
                    if not title: continue

                    # ì–¸ë¡ ì‚¬ ì°¾ê¸°
                    press_div = item.query_selector('.MgUUmf span')
                    press = press_div.inner_text() if press_div else "Google News"

                    # ì¸ë„¤ì¼ ì°¾ê¸°
                    img_tag = item.query_selector('img')
                    img = img_tag.get_attribute('src') if img_tag else None

                    articles.append({
                        "title": title,
                        "link": link,
                        "press": press,
                        "thumbnail": img,
                        "source": "Google"
                    })
                    count += 1
                except:
                    continue
                    
    except Exception as e:
        # êµ¬ê¸€ ê²€ìƒ‰ì´ ì‹¤íŒ¨í•´ë„ ë¡œê·¸ë§Œ ì°ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì„œë²„ ì•ˆ ì£½ìŒ)
        print(f"âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ ê±´ë„ˆëœ€ (ì‚¬ìœ : {e})")
        return []
        
    finally:
        if browser:
            try: browser.close()
            except: pass
            
    return articles


# --- 1. AI ëª¨ë¸ ë¡œë”© (ë¡œì»¬) ---
from transformers import AutoTokenizer, AutoModelForSequenceClassification
MODEL_PATH = os.environ.get("MODEL_DIRECTORY", "./my_fake_news_model") 

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    model.eval()
    print(f"âœ… AI ëª¨ë¸ ë¡œë”© ì„±ê³µ ({MODEL_PATH})")
except Exception as e:
    print(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    tokenizer = None
    model = None


# --- 2. ì–¸ë¡ ì‚¬ ì‹ ë¢°ë„ DB ---
MEDIA_TRUST_DB = {
    "KBS": {"rank": 1, "score": 42.2, "category": "ì‹ ë¢°ë„ 1ìœ„"},
    "MBC": {"rank": 2, "score": 30.5, "category": "ì‹ ë¢°ë„ 2ìœ„"},
    "YTN": {"rank": 3, "score": 22.8, "category": "ì‹ ë¢°ë„ 3ìœ„"},
}

def get_media_trust_score(publisher_name):
    for key in MEDIA_TRUST_DB.keys():
        if key in publisher_name: return MEDIA_TRUST_DB[key]
    return {"rank": None, "score": None, "category": "ìˆœìœ„ê¶Œ ì™¸"}

def extract_date_from_url(url):
    match = re.search(r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})', url)
    if match: return f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
    match_compact = re.search(r'/(\d{4})(\d{2})(\d{2})/', url)
    if match_compact: return f"{match_compact.group(1)}-{match_compact.group(2)}-{match_compact.group(3)}"
    return None


# --- 3. ê¸°ì‚¬ ì œëª©/ë³¸ë¬¸ í¬ë¡¤ë§ ---
def find_article_content(soup):
    title = ""
    text = ""
    
    title_tag = soup.select_one('h2.media_end_head_headline') or soup.select_one('h3.tit_view')
    if title_tag: title = title_tag.get_text().strip()
    if not title and soup.find('h1'): title = soup.find('h1').get_text().strip()
    if not title:
        og_title = soup.find('meta', {'property': 'og:title'})
        if og_title and og_title.get('content'): title = og_title['content'].strip()

    article_body = soup.select_one('div#dic_area') or soup.select_one('div.article_view') or soup.find('article')
    if article_body:
        text = article_body.get_text(separator=" ").strip()
    else:
        paragraphs = soup.find_all('p')
        text = " ".join(p.get_text().strip() for p in paragraphs if p.get_text())

    return title, text


# --- 4. AI ì˜ˆì¸¡ (ë¡œì»¬) ---
def get_fake_news_prediction(title, text):
    if not tokenizer or not model: return {"error": "AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
    input_text = f"{title} [SEP] {text}"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
    with torch.no_grad(): outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    prob_true = probs[0][0].item()
    prob_fake = probs[0][1].item()
    return {
        "prediction": "Fake" if prob_fake > prob_true else "True",
        "fake_percentage": round(prob_fake * 100, 2),
        "true_percentage": round(prob_true * 100, 2)
    }


# --- 5. ë„ë©”ì¸/ì¶œì²˜ ì²˜ë¦¬ ---
def get_domain_from_url(url):
    try:
        netloc = urlparse(url).netloc
        return netloc.replace('www.', '') if netloc.startswith('www.') else netloc
    except: return None

def find_publisher_name(soup, domain):
    try:
        og = soup.find('meta', {'property': 'og:site_name'})
        if og and og.get('content'): return og['content'].strip()
    except: pass
    return domain

def find_publish_date(soup, url):
    meta_targets = ['article:published_time', 'og:published_time', 'pubdate']
    for attr in meta_targets:
        # [ìˆ˜ì •] property=... ì™€ name=... ì„ attrs ë”•ì…”ë„ˆë¦¬ë¡œ ê°ì‹¸ì„œ ì¶©ëŒ ë°©ì§€
        tag = soup.find('meta', attrs={'property': attr}) or \
            soup.find('meta', attrs={'name': attr})
            
        if tag and tag.get('content'):
            try: return date_parser.parse(tag['content']).isoformat()
            except: continue
            
    url_date = extract_date_from_url(url)
    if url_date: return url_date
    return "ë‚ ì§œ ì°¾ê¸° ì‹¤íŒ¨"


# --- 6. Django View ---
@method_decorator(csrf_exempt, name='dispatch')
class AnalyzeView(APIView):
    throttle_classes = [AnonRateThrottle]

    def post(self, request, *args, **kwargs):
        # 1. URL ë° ë°ì´í„° ì¤€ë¹„
        try:
            data = json.loads(request.body)
            url_to_check = data.get('url')
        except: return JsonResponse({"success": False, "error": {"message": "ì˜ëª»ëœ ìš”ì²­"}}, status=400)

        domain = get_domain_from_url(url_to_check)
        
        # 2. í¬ë¡¤ë§ (Playwright)
        html = None
        browser = None
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page(user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
                page.goto(url_to_check, wait_until='domcontentloaded', timeout=90000)  
                html = page.content()
        except Exception as e:
            return JsonResponse({"success": False, "error": {"message": f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}"}}, status=500)
        finally:
            if browser: 
                try: browser.close() 
                except: pass
        
        if not html: return JsonResponse({"success": False, "error": {"message": "HTML ì¶”ì¶œ ì‹¤íŒ¨"}}, status=500)

        # 3. ë°ì´í„° ì¶”ì¶œ
        soup = BeautifulSoup(html, "html.parser")
        publisher_name = find_publisher_name(soup, domain)
        publish_date = find_publish_date(soup, url_to_check)
        title, text_content = find_article_content(soup)

        if not title or len(text_content) < 50:
            return JsonResponse({"success": False, "error": {"message": "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"}}, status=400)

        # 4. AI ë¶„ì„ ì‹¤í–‰
        ai_result = get_fake_news_prediction(title, text_content)
        gpt_result = get_gpt_prediction(title, text_content)
        
        # 5. ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ë° í•„í„°ë§ (â˜…â˜…â˜… ìˆ˜ì •ëœ ë¶€ë¶„ â˜…â˜…â˜…)
        related_articles = []
        keywords = gpt_result.get("keywords", "")
        
        if keywords:
            print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {keywords}")
            # ê°ê° 5ê°œì”© ë„‰ë„‰íˆ ê°€ì ¸ì˜´
            naver_raw = get_related_articles(keywords)
            google_raw = get_google_news(keywords)

            # ê²€ìƒ‰ ê²°ê³¼ í•©ì¹˜ê¸°
            all_articles = naver_raw + google_raw
            
            # [ì¤‘ë³µ ì œê±° ë¡œì§] í˜„ì¬ ê¸°ì‚¬ ì œëª©ê³¼ ìœ ì‚¬í•˜ë©´ ì œê±°
            # ì •ê·œí™”ëœ í˜„ì¬ ì œëª©
            current_title_norm = normalize_text(title)
            
            filtered_list = []
            for item in all_articles:
                item_title_norm = normalize_text(item['title'])
                
                # 1. ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ íŒ¨ìŠ¤
                if len(item_title_norm) < 2: continue
                
                # 2. í˜„ì¬ ê¸°ì‚¬ ì œëª©ì´ ê²€ìƒ‰ëœ ê¸°ì‚¬ ì œëª©ì— í¬í•¨ë˜ê±°ë‚˜, ê·¸ ë°˜ëŒ€ì¸ ê²½ìš° (ìœ ì‚¬ë„ ë†’ìŒ)
                # ì˜ˆ: "ë¹„íŠ¸ì½”ì¸ í­ë½" vs "ì˜¤ëŠ˜ ë¹„íŠ¸ì½”ì¸ í­ë½ ì¶©ê²©"
                if current_title_norm in item_title_norm or item_title_norm in current_title_norm:
                    print(f"ğŸš« ì¤‘ë³µ ì œì™¸ë¨: {item['title']}")
                    continue
                    
                filtered_list.append(item)

            # í•„í„°ë§ í›„ ìµœëŒ€ 5ê°œë§Œ ìë¥´ê¸° (ë„¤ì´ë²„ ìš°ì„ ìˆœìœ„ ìœ ì§€ë¥¼ ìœ„í•´ ì„ì§€ ì•Šê³  ìˆœì„œëŒ€ë¡œ)
            related_articles = filtered_list[:5]

        return JsonResponse({
            "success": True,
            "data": {
                "requested_url": url_to_check,
                "publisher_name": publisher_name,
                "published_date": publish_date,
                "scraped_title": title,
                "ai_prediction": ai_result,
                "media_trust": get_media_trust_score(publisher_name),
                "gpt_model": {
                    "score": gpt_result.get('score', 0),
                    "reason": gpt_result.get('reason', ""),
                    "prediction": gpt_result.get('prediction', ""),
                    "keywords": keywords
                },
                "related_articles": related_articles, # í•„í„°ë§ëœ ë¦¬ìŠ¤íŠ¸
                "cached": False
            }
        }, status=200)